# ===== LLM Service Configuration =====
# Service type: 'api', 'ollama', or 'mock'
# - 'api': Use Google Generative AI API directly (recommended, requires GEMINI_API_KEY)
# - 'ollama': Use local Ollama instance (requires Ollama running locally)
# - 'mock': Use simple HTML parser for testing (no API key needed)
LLM_SERVICE_TYPE=api

# ===== API Configuration (for LLM_SERVICE_TYPE=api) =====
# Get your API key from https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# Gemini model to use (default: gemini-1.5-flash)
# Options: gemini-1.5-flash, gemini-1.5-pro, gemini-2.0-flash-exp
GEMINI_MODEL=gemini-1.5-flash

# ===== Ollama Configuration (for LLM_SERVICE_TYPE=ollama) =====
# Base URL for Ollama API (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to use (e.g., 'llama2', 'llama3', 'mistral', 'codellama', etc.)
# Make sure the model is downloaded in Ollama first: ollama pull <model>
OLLAMA_MODEL=llama2

# ===== Mock Service (for testing) =====
# Set to 'true' to use mock service (for testing without API key)
# This will override LLM_SERVICE_TYPE
USE_MOCK_GEMINI=false

# Twitter API Configuration (optional - for Twitter/X URL support)
# Get your Bearer Token from https://developer.twitter.com/en/portal/dashboard
TWITTER_BEARER_TOKEN=your_twitter_bearer_token_here

# Input Configuration
OBSIDIAN_NOTES_PATH=./notes

# Processing Options
# Set to 'true' to preview results without modifying files
DRY_RUN=false
